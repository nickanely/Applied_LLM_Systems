{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-29T10:52:56.334010Z",
     "start_time": "2025-11-29T10:52:54.889583Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tree_sitter_python as tspython\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from tree_sitter import Language, Parser\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dataset Construction Summary\n",
    "\n",
    "I collected Python code from multiple GitHub repositories and used it as the base corpus for generating plagiarism test samples. For each selected function, I created two types of examples:\n",
    "\n",
    "**1. Positive (plagiarized) examples**\n",
    "I took original functions and applied realistic plagiarism-style transformations. These included renaming variables and functions, removing comments, changing formatting, and reordering independent statements while keeping the underlying logic identical. I validated each transformed sample using AST similarity and token-level overlap to ensure it remained structurally close to the original.\n",
    "\n",
    "**2. Negative (non-plagiarized) examples**\n",
    "I generated completely different Python solutions to unrelated algorithmic problems so that they shared no meaningful structure or logic with the original. I checked embedding similarity against both the original and the entire corpus to make sure these samples were genuinely dissimilar.\n",
    "\n",
    "I embedded the full corpus once, cached the embeddings, and used them during validation. The final output is a labeled dataset of more than 30 examples, stored in JSON, with metadata such as AST similarity, token Jaccard scores, and links back to the original code.\n",
    "\n",
    "\n",
    "# NOTE: I have not used structured output deliberately. While it is good practice to use them, in my scenario It was not working well, gave me overhead and some complications - thus I removed them"
   ],
   "id": "a863029900b8976a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T10:53:17.600681Z",
     "start_time": "2025-11-29T10:53:17.583425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# I have stored my keys in run configuration\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"API key missing. Fix your setup.\")\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "PY_LANGUAGE = Language(tspython.language())\n",
    "parser = Parser(PY_LANGUAGE)\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Get OpenAI embedding\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-large\",\n",
    "        input=text\n",
    "    )\n",
    "    return np.array(response.data[0].embedding)\n",
    "\n",
    "def get_ast_structure(code):\n",
    "    \"\"\"Extract AST node types as structural fingerprint\"\"\"\n",
    "    tree = parser.parse(bytes(code, \"utf8\"))\n",
    "    nodes = []\n",
    "    def traverse(node):\n",
    "        nodes.append(node.type)\n",
    "        for child in node.children:\n",
    "            traverse(child)\n",
    "    traverse(tree.root_node)\n",
    "    return ' '.join(nodes)\n",
    "\n",
    "def compute_ast_similarity(code1, code2):\n",
    "    \"\"\"Structural similarity via AST\"\"\"\n",
    "    ast1 = get_ast_structure(code1)\n",
    "    ast2 = get_ast_structure(code2)\n",
    "    emb1 = get_embedding(ast1)\n",
    "    emb2 = get_embedding(ast2)\n",
    "    return cosine_similarity([emb1], [emb2])[0][0]\n",
    "\n",
    "def compute_token_jaccard(code1, code2):\n",
    "    \"\"\"Token-level overlap\"\"\"\n",
    "    tree1 = parser.parse(bytes(code1, \"utf8\"))\n",
    "    tree2 = parser.parse(bytes(code2, \"utf8\"))\n",
    "    tokens1 = set(tree1.root_node.text.decode().split())\n",
    "    tokens2 = set(tree2.root_node.text.decode().split())\n",
    "    intersection = len(tokens1 & tokens2)\n",
    "    union = len(tokens1 | tokens2)\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def generate_plagiarized_variant(code):\n",
    "    \"\"\"Generate high-similarity plagiarized code\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Transform this Python code to be plagiarized (80-95% similar):\n",
    "\n",
    "Rules:\n",
    "1. Keep exact same algorithm and control flow\n",
    "2. Rename ALL variables/functions to different names\n",
    "3. Change whitespace/formatting\n",
    "4. Reorder independent statements ONLY\n",
    "5. Remove comments\n",
    "\n",
    "Original:\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "Return ONLY the transformed code, no markdown, no explanation.\"\"\"\n",
    "        }],\n",
    "        temperature=0.7,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "\n",
    "    result = response.choices[0].message.content.strip()\n",
    "    result = result.replace('```python', '').replace('```', '').strip()\n",
    "    return result\n",
    "\n",
    "def validate_plagiarized(original, generated):\n",
    "    \"\"\"Check if generated code is actually plagiarized\"\"\"\n",
    "    ast_sim = compute_ast_similarity(original, generated)\n",
    "    token_jac = compute_token_jaccard(original, generated)\n",
    "\n",
    "    is_valid = ast_sim >= 0.75 and token_jac >= 0.1\n",
    "    return is_valid, ast_sim, token_jac\n",
    "\n",
    "\n",
    "def generate_non_plagiarized_dsa(original_code):\n",
    "    \"\"\"Generate a completely different DSA problem solution\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Generate a COMPLETELY DIFFERENT Python DSA problem solution.\n",
    "\n",
    "RULES:\n",
    "1. Must solve a DIFFERENT algorithmic problem\n",
    "2. Use DIFFERENT data structures (if original uses array, use tree/graph/hash)\n",
    "3. Use DIFFERENT algorithmic paradigm (if original is greedy, use DP/backtracking/etc)\n",
    "4. Must be valid, executable Python code\n",
    "5. 30-200 lines\n",
    "\n",
    "AVOID any similarity to this:\n",
    "```python\n",
    "{original_code}\n",
    "```\n",
    "\n",
    "Return ONLY the Python code, no markdown, no explanation.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "\n",
    "    result = response.choices[0].message.content.strip()\n",
    "    result = result.replace('```python', '').replace('```', '').strip()\n",
    "    return result\n",
    "\n",
    "def validate_non_plagiarized(original, generated, corpus_embeddings, threshold=0.5):\n",
    "    \"\"\"Ensure generated code is dissimilar from both original AND entire corpus\"\"\"\n",
    "\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-large\",\n",
    "        input=[original, generated]\n",
    "    )\n",
    "    orig_sim = cosine_similarity(\n",
    "        [response.data[0].embedding],\n",
    "        [response.data[1].embedding]\n",
    "    )[0][0]\n",
    "\n",
    "    if orig_sim > threshold:\n",
    "        return False, orig_sim, 1.0\n",
    "\n",
    "    # Check against corpus\n",
    "    gen_emb = np.array(response.data[1].embedding).reshape(1, -1)\n",
    "    corpus_sims = cosine_similarity(gen_emb, corpus_embeddings)[0]\n",
    "    max_corpus_sim = corpus_sims.max()\n",
    "\n",
    "    if max_corpus_sim > threshold:\n",
    "        return False, orig_sim, max_corpus_sim\n",
    "\n",
    "    return True, orig_sim, max_corpus_sim\n",
    "\n",
    "\n",
    "def embed_batch(texts, batch_size=128):\n",
    "    \"\"\"Generate embeddings in batches using OpenAI API\"\"\"\n",
    "    all_embeds = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        resp = client.embeddings.create(model=\"text-embedding-3-large\", input=batch)\n",
    "        all_embeds.extend([d.embedding for d in resp.data])\n",
    "    return np.array(all_embeds, dtype=np.float32)\n",
    "\n",
    "def load_or_compute_embeddings(corpus, cache_path=\"indexes/embeddings.pkl\"):\n",
    "    \"\"\"Load cached embeddings or compute once and save\"\"\"\n",
    "    if Path(cache_path).exists():\n",
    "        print(f\"Loading embeddings from {cache_path}\")\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    print(\"Computing embeddings (one-time operation)...\")\n",
    "    embeddings = embed_batch(corpus['code'].tolist())\n",
    "\n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def generate_dataset(corpus_df, n_samples=30):\n",
    "    \"\"\"Generate plagiarism detection dataset\"\"\"\n",
    "\n",
    "    corpus_embeddings = load_or_compute_embeddings(corpus_df)\n",
    "\n",
    "    sample_df = corpus_df.sample(n=n_samples, random_state=42)\n",
    "\n",
    "    dataset = []\n",
    "    stats = {'pos_success': 0, 'pos_fail': 0, 'neg_success': 0, 'neg_fail': 0}\n",
    "\n",
    "    for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "        original = row['code']\n",
    "\n",
    "        # POSITIVE: Generate plagiarized variant\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                plagiarized = generate_plagiarized_variant(original)\n",
    "                is_valid, ast_sim, tok_jac = validate_plagiarized(original, plagiarized)\n",
    "\n",
    "                if is_valid:\n",
    "                    dataset.append({\n",
    "                        'code': plagiarized,\n",
    "                        'label': 1,\n",
    "                        'original_ref': original,\n",
    "                        'ast_similarity': float(ast_sim),\n",
    "                        'token_jaccard': float(tok_jac)\n",
    "                    })\n",
    "                    stats['pos_success'] += 1\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"  Attempt {attempt+1} failed: ast={ast_sim:.2f}, jac={tok_jac:.2f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "        else:\n",
    "            stats['pos_fail'] += 1\n",
    "\n",
    "        # NEGATIVE: Sample dissimilar corpus code\n",
    "        for attempt in range(5):\n",
    "            try:\n",
    "                non_plagiarized = generate_non_plagiarized_dsa(original)\n",
    "                is_valid, orig_sim, corpus_sim = validate_non_plagiarized(\n",
    "                    original, non_plagiarized, corpus_embeddings, threshold=0.6\n",
    "                )\n",
    "\n",
    "                if is_valid:\n",
    "                    dataset.append({\n",
    "                        'code': non_plagiarized,\n",
    "                        'label': 0,\n",
    "                        'original_ref': original,\n",
    "                        'original_similarity': float(orig_sim),\n",
    "                        'max_corpus_similarity': float(corpus_sim)\n",
    "                    })\n",
    "                    stats['neg_success'] += 1\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"  Negative attempt {attempt+1}: orig_sim={orig_sim:.2f}, corpus_sim={corpus_sim:.2f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Negative error: {e}\")\n",
    "        else:\n",
    "            stats['neg_fail'] += 1\n",
    "\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Positive: {stats['pos_success']}/{n_samples} (failed: {stats['pos_fail']})\")\n",
    "    print(f\"  Negative: {stats['neg_success']}/{n_samples} (failed: {stats['neg_fail']})\")\n",
    "    print(f\"  Total: {len(dataset)} samples\")\n",
    "\n",
    "    return dataset\n",
    "\n"
   ],
   "id": "d867cf58d8c86d01",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T09:33:06.053733Z",
     "start_time": "2025-11-29T09:27:15.455861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Usage\n",
    "df_corpus = pd.read_csv(\"data/reference_corpus.csv\")\n",
    "dataset = generate_dataset(df_corpus, n_samples=31)\n",
    "with open('data/test_dataset.json', 'w') as f:\n",
    "    json.dump(dataset, f, indent=2)"
   ],
   "id": "c14b18bf9400e72d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from indexes/embeddings.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempt 1 failed: ast=0.96, jac=0.08\n",
      "  Attempt 2 failed: ast=0.96, jac=0.08\n",
      "  Attempt 3 failed: ast=0.96, jac=0.08\n",
      "  Negative attempt 1: orig_sim=0.13, corpus_sim=0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 2/31 [00:23<05:25, 11.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Negative attempt 1: orig_sim=0.32, corpus_sim=0.62\n",
      "  Negative attempt 2: orig_sim=0.33, corpus_sim=0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 3/31 [00:40<06:28, 13.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Negative attempt 1: orig_sim=0.23, corpus_sim=0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/31 [00:48<05:12, 11.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Negative attempt 1: orig_sim=0.08, corpus_sim=0.61\n",
      "  Negative attempt 2: orig_sim=0.11, corpus_sim=0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 5/31 [01:06<05:53, 13.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Negative attempt 1: orig_sim=0.23, corpus_sim=0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/31 [01:24<04:23, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Negative attempt 1: orig_sim=0.25, corpus_sim=0.73\n",
      "  Negative attempt 2: orig_sim=0.25, corpus_sim=0.63\n",
      "  Negative attempt 3: orig_sim=0.23, corpus_sim=0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 11/31 [02:02<02:58,  8.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Negative attempt 1: orig_sim=0.16, corpus_sim=0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 13/31 [02:19<02:29,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Negative attempt 1: orig_sim=0.07, corpus_sim=0.62\n",
      "  Negative attempt 2: orig_sim=0.05, corpus_sim=0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 16/31 [02:46<01:58,  7.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Negative attempt 1: orig_sim=0.36, corpus_sim=0.80\n",
      "  Negative attempt 2: orig_sim=0.33, corpus_sim=0.70\n",
      "  Negative attempt 3: orig_sim=0.31, corpus_sim=0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 18/31 [03:10<02:05,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Negative attempt 1: orig_sim=0.26, corpus_sim=0.83\n",
      "  Negative attempt 2: orig_sim=0.25, corpus_sim=0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 19/31 [03:27<02:20, 11.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Negative attempt 1: orig_sim=0.12, corpus_sim=0.74\n",
      "  Negative attempt 2: orig_sim=0.16, corpus_sim=0.75\n",
      "  Negative attempt 3: orig_sim=0.15, corpus_sim=0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 20/31 [03:44<02:28, 13.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Negative attempt 1: orig_sim=0.31, corpus_sim=0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 24/31 [04:17<01:02,  8.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Negative attempt 1: orig_sim=0.12, corpus_sim=0.72\n",
      "  Negative attempt 2: orig_sim=0.13, corpus_sim=0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 25/31 [04:34<01:08, 11.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Negative attempt 1: orig_sim=0.22, corpus_sim=0.65\n",
      "  Negative attempt 2: orig_sim=0.20, corpus_sim=0.70\n",
      "  Negative attempt 3: orig_sim=0.22, corpus_sim=0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 27/31 [04:59<00:45, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempt 1 failed: ast=0.94, jac=0.09\n",
      "  Negative attempt 1: orig_sim=0.16, corpus_sim=0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 30/31 [05:28<00:09,  9.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Negative attempt 1: orig_sim=0.21, corpus_sim=0.69\n",
      "  Negative attempt 2: orig_sim=0.18, corpus_sim=0.63\n",
      "  Negative attempt 3: orig_sim=0.24, corpus_sim=0.74\n",
      "  Negative attempt 4: orig_sim=0.20, corpus_sim=0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [05:50<00:00, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Positive: 30/31 (failed: 1)\n",
      "  Negative: 31/31 (failed: 0)\n",
      "  Total: 61 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T09:33:06.103871Z",
     "start_time": "2025-11-29T09:33:06.101540Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "172a0e7ae2cda0b8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
