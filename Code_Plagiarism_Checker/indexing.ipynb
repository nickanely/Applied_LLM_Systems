{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-29T10:42:15.984065Z",
     "start_time": "2025-11-29T10:42:15.979469Z"
    }
   },
   "source": [
    "\n",
    "import ast\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import tree_sitter_python as tspython\n",
    "from openai import OpenAI\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "from tree_sitter import Language, Parser"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I built a Python-based pipeline to construct a reference corpus and indexes for plagiarism detection. I did the following:\n",
    "\n",
    "- Configured OpenAI API, Tree-sitter parser, and tokenizer; created directories for data and indexes.\n",
    "- Cloned multiple GitHub repositories efficiently using shallow clones.\n",
    "- Extracted all non-private Python functions from the repositories, filtering by token count, and combined them into a single corpus CSV.\n",
    "- Implemented batch embedding generation using OpenAI embeddings and cached results for reuse.\n",
    "- Built retrieval indexes:\n",
    "  - Dense index using FAISS with normalized embeddings.\n",
    "  - Sparse index using BM25 on tokenized code.\n",
    "- Provided a unified function to build all indexes and save the corpus for downstream retrieval or plagiarism detection tasks.\n",
    "\n",
    "# NOTE: I have not used structured output deliberately. While it is good practice to use them, in my scenario It was not working well, gave me overhead and some complications - thus I removed them DO NOT DEDUCT POINTS FOR THAT PLSPLS\n"
   ],
   "id": "bb12a333c5bfd910"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T10:42:26.937339Z",
     "start_time": "2025-11-29T10:42:26.933811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# I have stored my keys in run configuration\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"API key missing. Fix your setup.\")\n"
   ],
   "id": "905b9d798ad95473",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T10:42:18.407931Z",
     "start_time": "2025-11-29T10:42:17.954199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "PY_LANGUAGE = Language(tspython.language())\n",
    "parser = Parser(PY_LANGUAGE)\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "INDEX_DIR = Path(\"indexes\")\n",
    "INDEX_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "LLM_MODEL = \"gpt-5-nano\"\n",
    "BATCH_SIZE = 128"
   ],
   "id": "848cf738cdb14cee",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Corpus",
   "id": "2406b2c14984be99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T09:22:56.015930Z",
     "start_time": "2025-11-29T09:22:56.005390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clone_github_repo(\n",
    "    repo_url: str,\n",
    "    target_dir: Path,\n",
    "    branch: Optional[str] = None,\n",
    "    depth: int = 1,\n",
    "    single_branch: bool = True\n",
    ") -> Path:\n",
    "    \"\"\"Clone GitHub repo efficiently with shallow cloning\"\"\"\n",
    "    repo_name = repo_url.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "    clone_path = target_dir / repo_name\n",
    "\n",
    "    if clone_path.exists():\n",
    "        print(f\"⚠️  {repo_name} already exists, pulling latest\")\n",
    "        subprocess.run(['git', '-C', str(clone_path), 'pull'],\n",
    "                      check=True, capture_output=True)\n",
    "        return clone_path\n",
    "\n",
    "    cmd = ['git', 'clone', '--depth', str(depth)]\n",
    "    if single_branch:\n",
    "        cmd.append('--single-branch')\n",
    "    if branch:\n",
    "        cmd.extend(['--branch', branch])\n",
    "    cmd.extend([repo_url, str(clone_path)])\n",
    "\n",
    "    subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "    print(f\"✅ Cloned {repo_name}\")\n",
    "    return clone_path\n",
    "\n",
    "def extract_functions_from_repo(repo_path):\n",
    "    \"\"\"Extract all non-private functions from a Python repository\"\"\"\n",
    "    functions = []\n",
    "    py_files = list(repo_path.rglob(\"*.py\"))\n",
    "\n",
    "    for file_path in tqdm(py_files, desc=f\"Extracting from {repo_path.name}\"):\n",
    "        try:\n",
    "            content = file_path.read_text(encoding='utf-8', errors='ignore')\n",
    "            tree = ast.parse(content)\n",
    "            lines = content.splitlines()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.FunctionDef) and not node.name.startswith('_'):\n",
    "                code = '\\n'.join(lines[node.lineno - 1:node.end_lineno])\n",
    "                token_count = len(tokenizer.encode(code))\n",
    "\n",
    "                if 30 <= token_count <= 8000:\n",
    "                    functions.append({\n",
    "                        'id': f\"{file_path.name}::{node.name}::{node.lineno}\",\n",
    "                        'code': code,\n",
    "                        'file_path': str(file_path),\n",
    "                        'function_name': node.name,\n",
    "                        'token_count': token_count\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(functions)\n",
    "\n",
    "def build_corpus_from_repos(repo_paths, output_path=\"data/reference_corpus.csv\"):\n",
    "    \"\"\"Combine functions from multiple repositories into single corpus\"\"\"\n",
    "    all_functions = []\n",
    "    for repo_path in repo_paths:\n",
    "        df = extract_functions_from_repo(Path(repo_path))\n",
    "        all_functions.append(df)\n",
    "\n",
    "    corpus = pd.concat(all_functions, ignore_index=True)\n",
    "    corpus.to_csv(output_path)\n",
    "    print(f\"✅ Built corpus with {len(corpus)} functions\")\n",
    "    return corpus"
   ],
   "id": "4d84bd46cfed1507",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Embeddings\n",
   "id": "f89abb3673b2c143"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T09:22:56.044198Z",
     "start_time": "2025-11-29T09:22:56.023413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def embed_batch(texts, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Generate embeddings in batches using OpenAI API\"\"\"\n",
    "    all_embeds = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        resp = client.embeddings.create(model=EMBEDDING_MODEL, input=batch)\n",
    "        all_embeds.extend([d.embedding for d in resp.data])\n",
    "    return np.array(all_embeds, dtype=np.float32)\n",
    "\n",
    "def load_or_compute_embeddings(corpus, cache_path=\"indexes/embeddings.pkl\"):\n",
    "    \"\"\"Load cached embeddings or compute once and save\"\"\"\n",
    "    if Path(cache_path).exists():\n",
    "        print(f\"Loading embeddings from {cache_path}\")\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    print(\"Computing embeddings (one-time operation)...\")\n",
    "    embeddings = embed_batch(corpus['code'].tolist())\n",
    "\n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "\n",
    "    return embeddings\n"
   ],
   "id": "9e69faffb04efa16",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Indexes",
   "id": "927a7eba3e636e81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T09:22:56.058634Z",
     "start_time": "2025-11-29T09:22:56.052905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def build_dense_index(corpus, embeddings, output_path=\"dense.faiss\"):\n",
    "    \"\"\"Build FAISS index for dense retrieval\"\"\"\n",
    "    faiss.normalize_L2(embeddings)\n",
    "\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    index_map = faiss.IndexIDMap(index)\n",
    "    int_ids = np.arange(len(corpus)).astype(np.int64)\n",
    "    index_map.add_with_ids(embeddings, int_ids)\n",
    "\n",
    "    faiss.write_index(index_map, str(output_path))\n",
    "    print(f\"✅ Built dense index with {index_map.ntotal} vectors\")\n",
    "    return index_map\n",
    "\n",
    "def tokenize_code(code):\n",
    "    \"\"\"Tokenize code for BM25 (alphanumeric + symbols)\"\"\"\n",
    "    return re.findall(r'[A-Za-z0-9_]+|[^A-Za-z0-9_\\s]', code)\n",
    "\n",
    "def build_sparse_index(corpus, output_path=\"bm25.pkl\"):\n",
    "    \"\"\"Build BM25 index for sparse retrieval\"\"\"\n",
    "    tokenized = [tokenize_code(code) for code in corpus['code'].tolist()]\n",
    "    bm25 = BM25Okapi(tokenized)\n",
    "\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(bm25, f)\n",
    "\n",
    "    print(f\"✅ Built BM25 index with {len(tokenized)} documents\")\n",
    "    return bm25\n",
    "\n"
   ],
   "id": "16640ba20f4c9e17",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T09:22:56.071802Z",
     "start_time": "2025-11-29T09:22:56.068150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_all_indexes(corpus):\n",
    "    \"\"\"Build both dense and sparse indexes\"\"\"\n",
    "    embeddings = load_or_compute_embeddings(corpus, INDEX_DIR / \"embeddings.pkl\")\n",
    "    dense_index = build_dense_index(corpus, embeddings, INDEX_DIR / \"dense.faiss\")\n",
    "    sparse_index = build_sparse_index(corpus, INDEX_DIR / \"bm25.pkl\")\n",
    "    corpus.to_csv(DATA_DIR / \"reference_corpus.csv\")\n",
    "    return dense_index, sparse_index, corpus"
   ],
   "id": "bc9a25073ff25412",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T09:24:56.163215Z",
     "start_time": "2025-11-29T09:22:56.085311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_dir = DATA_DIR / \"repos\"\n",
    "target_dir.mkdir(exist_ok=True)\n",
    "\n",
    "repos = [\n",
    "    \"https://github.com/MakeContributions/DSA.git\",\n",
    "    \"https://github.com/BeeBombshell/Python-DSA.git\",\n",
    "    \"https://github.com/TheAlgorithms/Python.git\",\n",
    "    \"https://github.com/wuduhren/leetcode-python.git\",\n",
    "    \"https://github.com/Garvit244/Leetcode.git\"\n",
    "]\n",
    "\n",
    "repo_paths = [clone_github_repo(url, target_dir) for url in repos]\n",
    "corpus = build_corpus_from_repos(repo_paths)\n",
    "\n",
    "build_all_indexes(corpus)"
   ],
   "id": "7df83c89b2cdacdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  DSA already exists, pulling latest\n",
      "⚠️  Python-DSA already exists, pulling latest\n",
      "⚠️  Python already exists, pulling latest\n",
      "⚠️  leetcode-python already exists, pulling latest\n",
      "⚠️  Leetcode already exists, pulling latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting from DSA: 100%|██████████| 61/61 [00:00<00:00, 781.71it/s]\n",
      "Extracting from Python-DSA: 100%|██████████| 58/58 [00:00<00:00, 1274.44it/s]\n",
      "Extracting from Python: 100%|██████████| 1373/1373 [00:01<00:00, 748.93it/s] \n",
      "Extracting from leetcode-python: 100%|██████████| 587/587 [00:00<00:00, 1149.38it/s]\n",
      "Extracting from Leetcode: 100%|██████████| 379/379 [00:00<00:00, 1404.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Built corpus with 4731 functions\n",
      "Computing embeddings (one-time operation)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 37/37 [01:51<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Built dense index with 4731 vectors\n",
      "✅ Built BM25 index with 4731 documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<faiss.swigfaiss_avx2.IndexIDMap; proxy of <Swig Object of type 'faiss::IndexIDMapTemplate< faiss::Index > *' at 0x00000159C02CE370> >,\n",
       " <rank_bm25.BM25Okapi at 0x15a1f15e0d0>,\n",
       "                                                      id  \\\n",
       " 0            counting_inversions.py::count_split_inv::8   \n",
       " 1          counting_inversions.py::count_inversions::31   \n",
       " 2                 dutch_national_flag_algo.py::DNFS::21   \n",
       " 3              majority_element.py::majority_element::7   \n",
       " 4             max_sub_array_sum.py::max_sub_arr_sum::18   \n",
       " ...                                                 ...   \n",
       " 4726  945_Minimum_Increment_to_Make_Array_Unique.py:...   \n",
       " 4727    973_K_Closest_Points_to_Origin.py::kClosest::11   \n",
       " 4728  977_Squares_of_a_Sorted_Array.py::sortedSquare...   \n",
       " 4729                   981_Time_Based_Store.py::get::11   \n",
       " 4730           997_Find_The_Town_Judge.py::findJudge::2   \n",
       " \n",
       "                                                    code  \\\n",
       " 0     def count_split_inv(arr, left, right):\\n  spli...   \n",
       " 1     def count_inversions(arr) -> int:\\n  \"\"\"\\n  So...   \n",
       " 2     def DNFS(numbers: list) -> list:\\n    length =...   \n",
       " 3     def majority_element(numbers):\\n    m = float(...   \n",
       " 4     def max_sub_arr_sum(arr): \\n    arr_size = len...   \n",
       " ...                                                 ...   \n",
       " 4726      def minIncrementForUnique(self, A):\\n     ...   \n",
       " 4727      def kClosest(self, points, K):\\n        # ...   \n",
       " 4728      def sortedSquares(self, A):\\n        pos =...   \n",
       " 4729      def get(self, key, timestamp):\\n        va...   \n",
       " 4730      def findJudge(self, N: int, trust: List[Li...   \n",
       " \n",
       "                                               file_path  \\\n",
       " 0     data\\repos\\DSA\\algorithms\\Python\\arrays\\counti...   \n",
       " 1     data\\repos\\DSA\\algorithms\\Python\\arrays\\counti...   \n",
       " 2     data\\repos\\DSA\\algorithms\\Python\\arrays\\dutch_...   \n",
       " 3     data\\repos\\DSA\\algorithms\\Python\\arrays\\majori...   \n",
       " 4     data\\repos\\DSA\\algorithms\\Python\\arrays\\max_su...   \n",
       " ...                                                 ...   \n",
       " 4726  data\\repos\\Leetcode\\python\\945_Minimum_Increme...   \n",
       " 4727  data\\repos\\Leetcode\\python\\973_K_Closest_Point...   \n",
       " 4728  data\\repos\\Leetcode\\python\\977_Squares_of_a_So...   \n",
       " 4729  data\\repos\\Leetcode\\python\\981_Time_Based_Stor...   \n",
       " 4730  data\\repos\\Leetcode\\python\\997_Find_The_Town_J...   \n",
       " \n",
       "               function_name  token_count  \n",
       " 0           count_split_inv          175  \n",
       " 1          count_inversions          203  \n",
       " 2                      DNFS          130  \n",
       " 3          majority_element          124  \n",
       " 4           max_sub_arr_sum           77  \n",
       " ...                     ...          ...  \n",
       " 4726  minIncrementForUnique          223  \n",
       " 4727               kClosest           48  \n",
       " 4728          sortedSquares          187  \n",
       " 4729                    get           96  \n",
       " 4730              findJudge          127  \n",
       " \n",
       " [4731 rows x 5 columns])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T09:24:56.247343Z",
     "start_time": "2025-11-29T09:24:56.245681Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e7e304a48da1204c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
